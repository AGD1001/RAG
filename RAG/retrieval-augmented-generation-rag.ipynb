{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello everyone!\n\nI am Harsh Singhal, and I am excited to share with you this notebook on Retrieval Augmented Generation (RAG). This notebook is part of a linked series where we dive into the fascinating world of RAG and explore its applications.\n\n## About Me\n\nI am a passionate AI professional and I love exploring cutting-edge technologies and finding innovative solutions to real-world problems. NLP and its various applications have been a particular area of interest for me, and I am thrilled to share my insights and discoveries with you through this series.\n\n## Connect with Me\n\nIf you would like to connect with me or follow my work, feel free to connect with me on LinkedIn:\n\n[Harsh Singhal LinkedIn Profile](https://www.linkedin.com/in/harshsinghal)\n\n## Retrieval Augmented Generation (RAG)\n\nThe field of NLP has witnessed significant advancements in recent years, and RAG is one such exciting development. In this series of notebooks, we will cover the fundamentals of RAG, its architecture, and practical implementations. We will also work on some hands-on examples to grasp the concepts better.\n\nLet us embark on this journey together and explore the powerful capabilities of RAG!\n\nHappy learning! \n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:31:32.919906Z","iopub.execute_input":"2023-07-28T05:31:32.920557Z","iopub.status.idle":"2023-07-28T05:31:32.934931Z","shell.execute_reply.started":"2023-07-28T05:31:32.920513Z","shell.execute_reply":"2023-07-28T05:31:32.932529Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"In this Jupyter notebook, we will be working with the llama_index and langchain libraries to perform document indexing and retrieval using GPT-3.5-turbo, an advanced language model. The purpose of this notebook is to demonstrate how to set up the environment, load documents, and create an index for efficient document retrieval.\n\nBefore we proceed, please note that we will be using the OpenAI API to leverage the capabilities of the GPT-3.5-turbo model. As a security measure, remember never to reveal your API keys directly in code. Instead, use environment variables or other secure means to store sensitive information.\n\nWe will follow these steps:\n\n- Import necessary classes and functions from the llama_index and langchain libraries.\n- Set up the OpenAI API key using an environment variable and directly for demonstration purposes (please avoid doing this in production code).\n- Load data from the specified directory, where we assume the documents are stored. You may adjust the path according to your data location.\n- Initialize the LLMPredictor with the desired GPT-3.5-turbo model and temperature setting.\n- Create a ServiceContext using the initialized predictor.\n- Index the loaded documents using the created service context to enable efficient document retrieval.\n- Now that you have an overview of the tasks we'll be performing, let's proceed with the document loading and indexing process. Happy coding! ðŸš€","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Install llama_index which is a popular middleware used in many GenAI applications\n!pip install llama_index","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:23:08.262574Z","iopub.execute_input":"2023-07-28T05:23:08.263025Z","iopub.status.idle":"2023-07-28T05:23:29.116943Z","shell.execute_reply.started":"2023-07-28T05:23:08.262991Z","shell.execute_reply":"2023-07-28T05:23:29.115206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#          ___         \n#         / ()\\\\        \n#       _|_____|_       \n#      | | === | |      \n#      |_|  O  |_|        \n#       ||  O  ||         \n#       ||__*__||         \n#      |~ \\\\___/ ~|       \n#      /=\\\\ /=\\\\ /=\\\\     \n#______[_]_[_]_[_]_______\n\n\n# Import necessary classes and functions from the llama_index and langchain libraries\nfrom llama_index import (\n    GPTVectorStoreIndex,\n    SimpleDirectoryReader,\n    ServiceContext,\n    StorageContext,\n    LLMPredictor,\n    load_index_from_storage,\n)\nfrom langchain.chat_models import ChatOpenAI\n\n# Import the openai library and os module to set the API key\nimport openai\nimport os\n\n# SECURITY ALERT: Never reveal your API keys directly in code. Use environment variables or other secure means.\n# Here, we're setting the OpenAI API key both using an environment variable and directly (demonstration purposes only)\nos.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'\nopenai.api_key = 'YOUR_API_KEY'\n\n# Notify the user that the document loading process has begun\nprint(\"started the loading document process...\")\n\n# Read the data from the specified directory. Change './boiler_docs/' to your desired path.\ndocuments = SimpleDirectoryReader('/kaggle/input/aws-case-studies-and-blogs/').load_data()\n\n# Initialize the LLMPredictor with the desired GPT-3.5-turbo model and temperature setting\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n\n# Create a ServiceContext using the initialized predictor\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n\n# Notify the user that the indexing process has begun\nprint(\"started the indexing process...\")\n\n# Create an index using the loaded documents and the created service context\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:35:08.611426Z","iopub.execute_input":"2023-07-28T05:35:08.612846Z","iopub.status.idle":"2023-07-28T05:36:03.273624Z","shell.execute_reply.started":"2023-07-28T05:35:08.612796Z","shell.execute_reply":"2023-07-28T05:36:03.271944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the created index to the disk at the specified location\nprint(\"storing the index to disk\")\nindex.storage_context.persist(persist_dir=\"/kaggle/working/aws_case_documents_index\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:36:21.319389Z","iopub.execute_input":"2023-07-28T05:36:21.320751Z","iopub.status.idle":"2023-07-28T05:36:37.290496Z","shell.execute_reply.started":"2023-07-28T05:36:21.320693Z","shell.execute_reply":"2023-07-28T05:36:37.289086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notify the user that we are querying the index\nprint(\"Querying the index...\")\n\n# Query the index for the provided question and store the response\nresponse = index.as_query_engine().query(\"Write a detailed summary of AWS Personalize and in your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n\n# Print the received response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:37:43.328426Z","iopub.execute_input":"2023-07-28T05:37:43.328987Z","iopub.status.idle":"2023-07-28T05:37:56.253092Z","shell.execute_reply.started":"2023-07-28T05:37:43.328947Z","shell.execute_reply":"2023-07-28T05:37:56.251819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notify the user that we are querying the index\nprint(\"Querying the index...\")\n\n# Query the index for the provided question and store the response\nresponse = index.as_query_engine().query(\"Write a detailed summary of the use of Machine Learning by AWS customers in the Life Science industry. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n\n# Print the received response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:38:30.291176Z","iopub.execute_input":"2023-07-28T05:38:30.291650Z","iopub.status.idle":"2023-07-28T05:38:42.079468Z","shell.execute_reply.started":"2023-07-28T05:38:30.291615Z","shell.execute_reply":"2023-07-28T05:38:42.077941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notify the user that we are querying the index\nprint(\"Querying the index...\")\n\n# Query the index for the provided question and store the response\nresponse = index.as_query_engine().query(\"Write a detailed summary of the companies using AWS IoT services. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n\n# Print the received response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:39:14.177256Z","iopub.execute_input":"2023-07-28T05:39:14.177782Z","iopub.status.idle":"2023-07-28T05:39:28.249359Z","shell.execute_reply.started":"2023-07-28T05:39:14.177743Z","shell.execute_reply":"2023-07-28T05:39:28.247797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notify the user that we are querying the index\nprint(\"Querying the index...\")\n\n# Query the index for the provided question and store the response\nresponse = index.as_query_engine().query(\"Write a detailed summary of the competitive advantage of AWS services over other cloud vendors in the AI space. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n\n# Print the received response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:42:12.835341Z","iopub.execute_input":"2023-07-28T05:42:12.835895Z","iopub.status.idle":"2023-07-28T05:42:31.720875Z","shell.execute_reply.started":"2023-07-28T05:42:12.835855Z","shell.execute_reply":"2023-07-28T05:42:31.719455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notify the user that we are querying the index\nprint(\"Querying the index...\")\n\n# Query the index for the provided question and store the response\nresponse = index.as_query_engine().query(\"Write a detailed summary how GenAI applications can be used in the Fintech industry. In your response provided an executive summary after which you can present succinct bullet points? Remember to add formatting elements so that the output is easy to read and well-formatted. Use line breaks to improve formatting.\")\n\n# Print the received response\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:42:51.012849Z","iopub.execute_input":"2023-07-28T05:42:51.014626Z","iopub.status.idle":"2023-07-28T05:43:11.295101Z","shell.execute_reply.started":"2023-07-28T05:42:51.014536Z","shell.execute_reply":"2023-07-28T05:43:11.293837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can look at the details of the response object and go deeper into what sources of information were recalled.","metadata":{}},{"cell_type":"code","source":"dir(response)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:40:37.027888Z","iopub.execute_input":"2023-07-28T05:40:37.028370Z","iopub.status.idle":"2023-07-28T05:40:37.038816Z","shell.execute_reply.started":"2023-07-28T05:40:37.028334Z","shell.execute_reply":"2023-07-28T05:40:37.037329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response.source_nodes[0]","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:40:48.332832Z","iopub.execute_input":"2023-07-28T05:40:48.333410Z","iopub.status.idle":"2023-07-28T05:40:48.342116Z","shell.execute_reply.started":"2023-07-28T05:40:48.333366Z","shell.execute_reply":"2023-07-28T05:40:48.340678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}